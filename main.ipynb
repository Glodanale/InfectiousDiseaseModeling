{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project/\n",
    "# â”œâ”€â”€ main.py                 â†’ experiment loop entry point\n",
    "# â”œâ”€â”€ dataloader.py            â†’ data preprocessing, loader, scaling\n",
    "# â”œâ”€â”€ models.py               â†’ HybridODE, LSTMHybrid\n",
    "# â”œâ”€â”€ train.py                â†’ training + early stopping\n",
    "# â”œâ”€â”€ visualize.py            â†’ metrics, plots\n",
    "# â””â”€â”€ config.py               â†’ param_grid + experiment combos\n",
    "\n",
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from config import param_grid\n",
    "from dataloader import SEIRDDataLoader, get_split_strategy\n",
    "from models import HybridODE\n",
    "from train import train_model\n",
    "from visualize import visualize_supervised, visualize_trajectory\n",
    "\n",
    "EXPERIMENT_LOG_PATH = \"experiment_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(run_id, config, exp_folder):\n",
    "    log_exists = os.path.exists(EXPERIMENT_LOG_PATH)\n",
    "    with open(EXPERIMENT_LOG_PATH, mode='a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"run_id\", \"folder\"] + list(config.keys()))\n",
    "        if not log_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\"run_id\": run_id, \"folder\": exp_folder, **config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(config, dataloader, train_loader, val_loader, folder_name):\n",
    "    save_dir = os.path.join(\"results\", folder_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = HybridODE(\n",
    "        input_dim=dataloader.num_features,\n",
    "        hidden_dim=64,\n",
    "        num_layers=2,\n",
    "        output_dim=dataloader.num_features,\n",
    "        solver=config[\"solver\"],\n",
    "        sensitivity=config[\"sensitivity\"]\n",
    "    ).to(device)\n",
    "\n",
    "    trained_model, test_loader, _ = train_model({\n",
    "        \"dataset_path\": \"../../SEIR_CSV.csv\",\n",
    "        \"sequence_length\": config[\"sequence_length\"],\n",
    "        \"scaler_path\": os.path.join(save_dir, \"scaler.pkl\"),\n",
    "        \"split_strategy\": config[\"split_strategy\"],\n",
    "        \"batch_size\": config[\"batch_size\"],\n",
    "        \"learning_rate\": config[\"learning_rate\"],\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"solver\": config[\"solver\"],\n",
    "        \"sensitivity\": config[\"sensitivity\"],\n",
    "        \"epochs\": config[\"epochs\"],\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.2,\n",
    "        \"save_dir\": save_dir\n",
    "    })\n",
    "\n",
    "    visualize_supervised(\n",
    "        trained_model,\n",
    "        test_loader,\n",
    "        scaler_path=os.path.join(save_dir, \"scaler.pkl\"),\n",
    "        output_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    visualize_trajectory(\n",
    "        trained_model,\n",
    "        test_loader,\n",
    "        scaler_path=os.path.join(save_dir, \"scaler.pkl\"),\n",
    "        output_dir=os.path.join(save_dir, \"trajectory_plots\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, run_id):\n",
    "    set_seed(config.get(\"seed\", 42))  # default to 42 if not specified\n",
    "    exp_folder = f\"exp_{run_id:03d}\"\n",
    "    save_dir = os.path.join(\"results\", exp_folder)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Running {exp_folder}:\")\n",
    "    print(config)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # Log experiment config\n",
    "    log_experiment(run_id, config, exp_folder)\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    # Prepare data\n",
    "    dataloader = SEIRDDataLoader(\n",
    "        dataset_path='../../SEIR_CSV.csv',\n",
    "        sequence_length=config['sequence_length'],\n",
    "        scaler_path=os.path.join(save_dir, 'scaler.pkl')\n",
    "    )\n",
    "    data = dataloader.data\n",
    "\n",
    "    # Get loaders based on split strategy\n",
    "    split_result = get_split_strategy(\n",
    "        data, strategy=config[\"split_strategy\"],\n",
    "        sequence_length=config[\"sequence_length\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        val_size=0.2,\n",
    "        test_size=0.2,\n",
    "        window_size=config.get(\"window_size\", 200),\n",
    "        horizon=config.get(\"horizon\", 25),\n",
    "        stride=config.get(\"stride\", 25)\n",
    "    )\n",
    "\n",
    "    # Handle multi-fold splits\n",
    "    if isinstance(split_result, list):\n",
    "        for fold, (train_loader, val_loader, _) in enumerate(split_result):\n",
    "            print(f\"â†’ Fold {fold + 1}/{len(split_result)}\")\n",
    "            fold_suffix = f\"_fold{fold}\"\n",
    "            train_single_model(config, dataloader, train_loader, val_loader, f\"{exp_folder}{fold_suffix}\")\n",
    "    else:\n",
    "        train_loader, val_loader, test_loader = split_result\n",
    "        train_single_model(config, dataloader, train_loader, val_loader, exp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n",
      "\n",
      "==============================\n",
      "Running exp_000:\n",
      "{'split_strategy': 'train_val_test', 'solver': 'tsit5', 'sensitivity': 'adjoint', 'learning_rate': 0.01, 'batch_size': 10, 'window_size': 200, 'horizon': 25, 'stride': 25, 'sequence_length': 800, 'epochs': 2, 'seed': 42}\n",
      "==============================\n",
      "âœ… Scaler loaded successfully!\n",
      "âœ… Scaler saved successfully!\n",
      "Training with: cuda\n",
      "âœ… Scaler loaded successfully!\n",
      "âœ… Scaler saved successfully!\n",
      "Epoch 1, Train Loss: 0.159292\n",
      "Epoch 1, Validation Loss: 0.022331\n",
      "Epoch 2, Train Loss: 0.013539\n",
      "Epoch 2, Validation Loss: 0.002847\n",
      "âœ… Best model saved to results/exp_000/best_model.pth\n",
      "ðŸ“Š Training log saved to results/exp_000/training_log.csv\n",
      "  Feature           MSE         RMSE          MAE  R2 Score\n",
      "0       S  3.471269e+06  1863.134155  1523.373779  0.179093\n",
      "1       E  4.972750e+04   222.996643   177.451782 -0.581595\n",
      "2     Ins  2.769577e+03    52.626770    42.198692 -0.549595\n",
      "3      Is  3.487679e+04   186.753281   155.158997 -0.439883\n",
      "4      Ia  2.317012e+04   152.217361   125.022224 -0.485541\n",
      "5       D  2.226567e+03    47.186516    38.664898 -0.732222\n",
      "6       R  2.283061e+06  1510.980103  1237.972290 -0.707833\n",
      "  Feature  Wasserstein Distance\n",
      "0       S            293.154859\n",
      "1       E             94.458115\n",
      "2     Ins             22.246594\n",
      "3      Is             74.647828\n",
      "4      Ia             45.557355\n",
      "5       D             16.241090\n",
      "6       R            502.898622\n",
      "\n",
      "==============================\n",
      "Running exp_001:\n",
      "{'split_strategy': 'train_val_test', 'solver': 'tsit5', 'sensitivity': 'adjoint', 'learning_rate': 0.01, 'batch_size': 25, 'window_size': 200, 'horizon': 25, 'stride': 25, 'sequence_length': 800, 'epochs': 2, 'seed': 42}\n",
      "==============================\n",
      "âš ï¸ Scaler not found. Creating a new one.\n",
      "âœ… Scaler saved successfully!\n",
      "Training with: cuda\n",
      "âœ… Scaler loaded successfully!\n",
      "âœ… Scaler saved successfully!\n",
      "Epoch 1, Train Loss: 0.351928\n",
      "Epoch 1, Validation Loss: 0.142786\n",
      "Epoch 2, Train Loss: 0.035855\n",
      "Epoch 2, Validation Loss: 0.007327\n",
      "âœ… Best model saved to results/exp_001/best_model.pth\n",
      "ðŸ“Š Training log saved to results/exp_001/training_log.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 25 and the array at index 1 has size 15",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run_id, combo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(combinations):\n\u001b[32m      8\u001b[39m     config = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_grid.keys(), combo))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(config, run_id)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     train_loader, val_loader, test_loader = split_result\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_single_model\u001b[39m\u001b[34m(config, dataloader, train_loader, val_loader, folder_name)\u001b[39m\n\u001b[32m      7\u001b[39m model = HybridODE(\n\u001b[32m      8\u001b[39m     input_dim=dataloader.num_features,\n\u001b[32m      9\u001b[39m     hidden_dim=\u001b[32m64\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     sensitivity=config[\u001b[33m\"\u001b[39m\u001b[33msensitivity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m ).to(device)\n\u001b[32m     16\u001b[39m trained_model, test_loader, _ = train_model({\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_path\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m../../SEIR_CSV.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msequence_length\u001b[39m\u001b[33m\"\u001b[39m: config[\u001b[33m\"\u001b[39m\u001b[33msequence_length\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msave_dir\u001b[39m\u001b[33m\"\u001b[39m: save_dir\n\u001b[32m     31\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mvisualize_supervised\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscaler.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m visualize_trajectory(\n\u001b[32m     41\u001b[39m     trained_model,\n\u001b[32m     42\u001b[39m     test_loader,\n\u001b[32m     43\u001b[39m     scaler_path=os.path.join(save_dir, \u001b[33m\"\u001b[39m\u001b[33mscaler.pkl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     44\u001b[39m     output_dir=os.path.join(save_dir, \u001b[33m\"\u001b[39m\u001b[33mtrajectory_plots\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sandia/STAT_5910/NODE_alex/visualize.py:40\u001b[39m, in \u001b[36mvisualize_supervised\u001b[39m\u001b[34m(model, test_loader, scaler_path, output_dir)\u001b[39m\n\u001b[32m     37\u001b[39m         all_actual.append(actual)\n\u001b[32m     38\u001b[39m         all_predicted.append(prediction)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m actual_flat, predicted_flat = \u001b[43mflatten_and_align_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_actual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_predicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m actual_flat = scaler.inverse_transform(actual_flat)\n\u001b[32m     43\u001b[39m predicted_flat = scaler.inverse_transform(predicted_flat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sandia/STAT_5910/NODE_alex/visualize.py:14\u001b[39m, in \u001b[36mflatten_and_align_sequences\u001b[39m\u001b[34m(actual, predicted)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_and_align_sequences\u001b[39m(actual, predicted):\n\u001b[32m     13\u001b[39m     actual_flat = np.concatenate(actual, axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     predicted_flat = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m actual_flat.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m     16\u001b[39m         actual_flat = actual_flat.reshape(-\u001b[32m1\u001b[39m, actual_flat.shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 25 and the array at index 1 has size 15"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"ðŸš€ Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
    "    combinations = list(product(*param_grid.values()))\n",
    "    for run_id, combo in enumerate(combinations):\n",
    "        config = dict(zip(param_grid.keys(), combo))\n",
    "        run_experiment(config, run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
